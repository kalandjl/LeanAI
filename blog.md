# LeanAI: My First ML Project     

## Inspiration 
The inspiration for this project came from a personal hobby of mine - weightlifting. Working out has been central to my personal developement in the past few years; but more importantly dieting to lose or gain weight was a practice that required me to build self-accountability and consistency. A key metric to dictate if one should "bulk" or "cut" is body fat % - in simple terms, how much of your body is composed of fat tissue. Given this percentage, one can make their dieting choices in a scientific, numerical manner, as opposed to purley based on emotions or feeling. Being aware of the importance of body fat %, I made my own post in r/guessmybf, where users estimate your body fat based on uploaded physique photos. A few weeks later, I made the decision to start studying machine learning, and spending a week watching videos, I was ready to translate my knowledge into a project; the first thing that came to mind was a model that would predict one's body fat % given a photo. 

## Gathering The Data

My first approach to culminating a dataset to train the model on was to find medical grade bodyfat measurements online, accompagnied with physique photos. However, after much searching I found that such data existed in limited quanties, far from enough to train a sufficient model. In this moment, I decided to sacrifice the medical-based precision of my model: rather opting to train from human estimates instead of medical body fat scans (DEXA, MRI). Luckily, sourcing such data was much easier, and I already new exactly where to pull it from: reddit. The ethical concerns of training on public data are pertinent in this situation, and my commitment while completing this project was to train the model on the given data in an anonymous and apersonal manner. The dataset was used exclusively for the non-commercial purpose of training this portfolio project model and has not been publicly redistributed. 

My approach to culminating the data was through the pip package "praw", which allowed for asynchronous API calls to the reddict developer interface. I gathered posts from 2 subreddits: "r/guessmybf", sorted by highest upvoted posts, and "r/bulkorcut", queried by the string "body fat". After gathering all posts, translating them to a trainable .csv file was a hurdle itself. Given the dependent variable - body fat % - was represented by human estimation, I had to make sure the estimations I was using were accurate and serious. At first, I simply queried each posts commment's by any number follow by a percentage symbol (eg. 19%), and then taking the average of all estimates as the dependent variable for that given post. However, I quickly found out the faliability in such a method, as many of these posts consisted of sarcastic or humourous replies that skewed the data massively. To solve this, I fed each post's comment array into OpenAI's ChatGPT 4.1 API, given the following prompt: 
```
"You extract body fat % predictions from a Python array of Reddit comments. "
"Return an array of the predictions in integer form. Ignore arbitrary numbers. "
"If the user gives a range (like 12-15%), return the mean. "
"Output only a valid Python list of integers, like [12, 18, 22]."
```
This prompt required testing to get right, but ultimatley resulted in a more accurate and factually based model.