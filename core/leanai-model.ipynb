{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "path = Path('data/archive/bodyfat_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>meanPrediction</th>\n",
       "      <th>medianPrediction</th>\n",
       "      <th>bfPredictions</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leanest ive ever been. Never seen veins like this before.</td>\n",
       "      <td>https://www.reddit.com/gallery/1b6k5jh</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[8]</td>\n",
       "      <td>https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-hibkcr8hfdmc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4604d036777deffcf28c2e2f396b19f77ac54c30</td>\n",
       "      <td>https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-cvvyjr8hfdmc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbc6e7207b1aae8893ecb76d1a68adc87fcd7aa2</td>\n",
       "      <td>https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-kev0ir8hfdmc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cf034949f12a0b62655c3dbcd0d77d4418c64fe</td>\n",
       "      <td>https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-yur9ot8hfdmc1.jpg?width=476&amp;format=pjpg&amp;auto=webp&amp;s=5af3f2a2258f511a1606651727635f31f55f4d82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Let me know. 78kg :)</td>\n",
       "      <td>https://i.redd.it/4occeq9wdd2c1.jpg</td>\n",
       "      <td>9.80</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[9, 10, 10, 9, 11]</td>\n",
       "      <td>https://i.redd.it/4occeq9wdd2c1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is my bf% I believe it’s around 13-14%</td>\n",
       "      <td>https://i.redd.it/lvfmowq0zhh91.jpg</td>\n",
       "      <td>14.33</td>\n",
       "      <td>13.5</td>\n",
       "      <td>[13, 19, 14, 14, 13, 13]</td>\n",
       "      <td>https://i.redd.it/lvfmowq0zhh91.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25F | 4'11\" | 107 lbs</td>\n",
       "      <td>https://www.reddit.com/gallery/1e30z2f</td>\n",
       "      <td>23.33</td>\n",
       "      <td>24.0</td>\n",
       "      <td>[20, 26, 24]</td>\n",
       "      <td>https://preview.redd.it/25f-411-107-lbs-v0-4lmyvzck8hcd1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ad8d28fdda2a0c57664d858e31e13889dfb008d</td>\n",
       "      <td>https://preview.redd.it/25f-411-107-lbs-v0-huczvrhk8hcd1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2445bde27112796d8e09d8825a1cb03d7bb83108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bodyfat?</td>\n",
       "      <td>https://www.reddit.com/gallery/1ktwao9</td>\n",
       "      <td>17.80</td>\n",
       "      <td>18.0</td>\n",
       "      <td>[18, 16, 18, 15, 22]</td>\n",
       "      <td>https://preview.redd.it/8ekhv0d4xl2f1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dfaa3a2d96c714e36a6b2579ad6559b098e145c7</td>\n",
       "      <td>https://preview.redd.it/rwkt50d4xl2f1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d4a08637bbd189e9f44f47dfe20948be74f85d4</td>\n",
       "      <td>https://preview.redd.it/vau412d4xl2f1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=610a51fef6107a8c7635afdc1722d01e620bf86b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>[GMBF] 6'1 220LBS Any estimates appreciated!</td>\n",
       "      <td>https://i.redd.it/z543jfquva731.jpg</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>[12]</td>\n",
       "      <td>https://i.redd.it/z543jfquva731.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>[GMBF] (M/22/6'2\"/195lbs)</td>\n",
       "      <td>https://i.imgur.com/PY44dK5.jpg</td>\n",
       "      <td>10.50</td>\n",
       "      <td>10.5</td>\n",
       "      <td>[11, 9, 10, 12]</td>\n",
       "      <td>https://i.imgur.com/PY44dK5.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>[GMBF] M/26/6'1/191lbs - down from 245lbs</td>\n",
       "      <td>https://i.imgur.com/pWGT7nV.jpg</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[15]</td>\n",
       "      <td>https://i.imgur.com/pWGT7nV.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>[GMBF] (M/26/5’10”/153lbs to 168lbs) What would you estimate my bf% to be before and after 4 months of (hopefully lean) bulking?</td>\n",
       "      <td>https://i.redd.it/lc2q2svumrt11.jpg</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>[10, 14]</td>\n",
       "      <td>https://i.redd.it/lc2q2svumrt11.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Thoughts?</td>\n",
       "      <td>https://i.redd.it/3yube1tanu1f1.jpeg</td>\n",
       "      <td>13.50</td>\n",
       "      <td>13.5</td>\n",
       "      <td>[14, 13]</td>\n",
       "      <td>https://i.redd.it/3yube1tanu1f1.jpeg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>806 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                title  \\\n",
       "0                                                                           Leanest ive ever been. Never seen veins like this before.   \n",
       "1                                                                                                                Let me know. 78kg :)   \n",
       "2                                                                                         What is my bf% I believe it’s around 13-14%   \n",
       "3                                                                                                              25F | 4'11\" | 107 lbs    \n",
       "4                                                                                                                            Bodyfat?   \n",
       "..                                                                                                                                ...   \n",
       "801                                                                                      [GMBF] 6'1 220LBS Any estimates appreciated!   \n",
       "802                                                                                                         [GMBF] (M/22/6'2\"/195lbs)   \n",
       "803                                                                                         [GMBF] M/26/6'1/191lbs - down from 245lbs   \n",
       "804  [GMBF] (M/26/5’10”/153lbs to 168lbs) What would you estimate my bf% to be before and after 4 months of (hopefully lean) bulking?   \n",
       "805                                                                                                                         Thoughts?   \n",
       "\n",
       "                                        url  meanPrediction  medianPrediction  \\\n",
       "0    https://www.reddit.com/gallery/1b6k5jh            8.00               8.0   \n",
       "1       https://i.redd.it/4occeq9wdd2c1.jpg            9.80              10.0   \n",
       "2       https://i.redd.it/lvfmowq0zhh91.jpg           14.33              13.5   \n",
       "3    https://www.reddit.com/gallery/1e30z2f           23.33              24.0   \n",
       "4    https://www.reddit.com/gallery/1ktwao9           17.80              18.0   \n",
       "..                                      ...             ...               ...   \n",
       "801     https://i.redd.it/z543jfquva731.jpg           12.00              12.0   \n",
       "802         https://i.imgur.com/PY44dK5.jpg           10.50              10.5   \n",
       "803         https://i.imgur.com/pWGT7nV.jpg           15.00              15.0   \n",
       "804     https://i.redd.it/lc2q2svumrt11.jpg           12.00              12.0   \n",
       "805    https://i.redd.it/3yube1tanu1f1.jpeg           13.50              13.5   \n",
       "\n",
       "                bfPredictions  \\\n",
       "0                         [8]   \n",
       "1          [9, 10, 10, 9, 11]   \n",
       "2    [13, 19, 14, 14, 13, 13]   \n",
       "3                [20, 26, 24]   \n",
       "4        [18, 16, 18, 15, 22]   \n",
       "..                        ...   \n",
       "801                      [12]   \n",
       "802           [11, 9, 10, 12]   \n",
       "803                      [15]   \n",
       "804                  [10, 14]   \n",
       "805                  [14, 13]   \n",
       "\n",
       "                                                                                                                                                                            image_1  \\\n",
       "0    https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-hibkcr8hfdmc1.jpg?width=640&crop=smart&auto=webp&s=4604d036777deffcf28c2e2f396b19f77ac54c30   \n",
       "1                                                                                                                                               https://i.redd.it/4occeq9wdd2c1.jpg   \n",
       "2                                                                                                                                               https://i.redd.it/lvfmowq0zhh91.jpg   \n",
       "3                                            https://preview.redd.it/25f-411-107-lbs-v0-4lmyvzck8hcd1.jpg?width=640&crop=smart&auto=webp&s=8ad8d28fdda2a0c57664d858e31e13889dfb008d   \n",
       "4                                                               https://preview.redd.it/8ekhv0d4xl2f1.jpg?width=640&crop=smart&auto=webp&s=dfaa3a2d96c714e36a6b2579ad6559b098e145c7   \n",
       "..                                                                                                                                                                              ...   \n",
       "801                                                                                                                                             https://i.redd.it/z543jfquva731.jpg   \n",
       "802                                                                                                                                                 https://i.imgur.com/PY44dK5.jpg   \n",
       "803                                                                                                                                                 https://i.imgur.com/pWGT7nV.jpg   \n",
       "804                                                                                                                                             https://i.redd.it/lc2q2svumrt11.jpg   \n",
       "805                                                                                                                                            https://i.redd.it/3yube1tanu1f1.jpeg   \n",
       "\n",
       "                                                                                                                                                                            image_2  \\\n",
       "0    https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-cvvyjr8hfdmc1.jpg?width=640&crop=smart&auto=webp&s=bbc6e7207b1aae8893ecb76d1a68adc87fcd7aa2   \n",
       "1                                                                                                                                                                               NaN   \n",
       "2                                                                                                                                                                               NaN   \n",
       "3                                            https://preview.redd.it/25f-411-107-lbs-v0-huczvrhk8hcd1.jpg?width=640&crop=smart&auto=webp&s=2445bde27112796d8e09d8825a1cb03d7bb83108   \n",
       "4                                                               https://preview.redd.it/rwkt50d4xl2f1.jpg?width=640&crop=smart&auto=webp&s=9d4a08637bbd189e9f44f47dfe20948be74f85d4   \n",
       "..                                                                                                                                                                              ...   \n",
       "801                                                                                                                                                                             NaN   \n",
       "802                                                                                                                                                                             NaN   \n",
       "803                                                                                                                                                                             NaN   \n",
       "804                                                                                                                                                                             NaN   \n",
       "805                                                                                                                                                                             NaN   \n",
       "\n",
       "                                                                                                                                                                            image_3  \\\n",
       "0    https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-kev0ir8hfdmc1.jpg?width=640&crop=smart&auto=webp&s=3cf034949f12a0b62655c3dbcd0d77d4418c64fe   \n",
       "1                                                                                                                                                                               NaN   \n",
       "2                                                                                                                                                                               NaN   \n",
       "3                                                                                                                                                                               NaN   \n",
       "4                                                               https://preview.redd.it/vau412d4xl2f1.jpg?width=640&crop=smart&auto=webp&s=610a51fef6107a8c7635afdc1722d01e620bf86b   \n",
       "..                                                                                                                                                                              ...   \n",
       "801                                                                                                                                                                             NaN   \n",
       "802                                                                                                                                                                             NaN   \n",
       "803                                                                                                                                                                             NaN   \n",
       "804                                                                                                                                                                             NaN   \n",
       "805                                                                                                                                                                             NaN   \n",
       "\n",
       "                                                                                                                                                                             image_4  \\\n",
       "0    https://preview.redd.it/leanest-ive-ever-been-never-seen-veins-like-this-before-v0-yur9ot8hfdmc1.jpg?width=476&format=pjpg&auto=webp&s=5af3f2a2258f511a1606651727635f31f55f4d82   \n",
       "1                                                                                                                                                                                NaN   \n",
       "2                                                                                                                                                                                NaN   \n",
       "3                                                                                                                                                                                NaN   \n",
       "4                                                                                                                                                                                NaN   \n",
       "..                                                                                                                                                                               ...   \n",
       "801                                                                                                                                                                              NaN   \n",
       "802                                                                                                                                                                              NaN   \n",
       "803                                                                                                                                                                              NaN   \n",
       "804                                                                                                                                                                              NaN   \n",
       "805                                                                                                                                                                              NaN   \n",
       "\n",
       "     image_5  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "..       ...  \n",
       "801      NaN  \n",
       "802      NaN  \n",
       "803      NaN  \n",
       "804      NaN  \n",
       "805      NaN  \n",
       "\n",
       "[806 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, numpy as np, pandas as pd\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must downlaod all the images from our dataframe using simple http fetching. The images are stores as row_image_number.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_all_images(df, output_dir=\"images\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    image_cols = [f\"image_{i}\" for i in range(1, 6)]\n",
    "    \n",
    "    seen_urls = set()\n",
    "    image_count = 0\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        for col in image_cols:\n",
    "            url = row.get(col)\n",
    "            if isinstance(url, str) and url.startswith(\"http\") and url not in seen_urls:\n",
    "                try:\n",
    "                    response = requests.get(url, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        file_ext = url.split('.')[-1].split('?')[0]\n",
    "                        file_name = f\"{idx}_{col}.{file_ext}\"\n",
    "                        file_path = os.path.join(output_dir, file_name)\n",
    "                        with open(file_path, \"wb\") as f:\n",
    "                            f.write(response.content)\n",
    "                        seen_urls.add(url)\n",
    "                        image_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "    print(f\"\\nDownloaded {image_count} unique images to '{output_dir}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps our original dataset, where each row could have up to 5 images, into a dataset where each image has it's own row - and a coresponding body fat %. This prepares the data to be put through our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_csv(df, output_csv=\"image_labels.csv\", label_col=\"meanPrediction\", image_prefix=\"image_\", output_dir=\"images\"):\n",
    "    # Ensure column names are stripped of whitespace\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    image_cols = [col for col in df.columns if col.startswith(image_prefix)]\n",
    "    records = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        label = row[label_col]\n",
    "        for col in image_cols:\n",
    "            url = row.get(col)\n",
    "            if isinstance(url, str) and url.startswith(\"http\"):\n",
    "                ext = url.split('.')[-1].split('?')[0].lower()\n",
    "                ext = ext if ext in ['jpg', 'jpeg', 'png', 'webp'] else 'jpg'\n",
    "                filename = f\"{idx}_{col}.{ext}\"\n",
    "                records.append({\"filename\": filename, \"target\": label})\n",
    "    \n",
    "    df_out = pd.DataFrame(records)\n",
    "    df_out.to_csv(output_csv, index=False)\n",
    "    print(f\"Created {output_csv} with {len(df_out)} labeled images\")\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [06:16<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloaded 1578 unique images to 'images/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_all_images(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created image_labels.csv with 1597 labeled images\n"
     ]
    }
   ],
   "source": [
    "df_labels = create_regression_csv(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our images have downloaded and are stored in a much more simple dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_image_1.jpg</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_image_2.jpg</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_image_3.jpg</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_image_4.jpg</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_image_1.jpg</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  target\n",
       "0  0_image_1.jpg     8.0\n",
       "1  0_image_2.jpg     8.0\n",
       "2  0_image_3.jpg     8.0\n",
       "3  0_image_4.jpg     8.0\n",
       "4  1_image_1.jpg     9.8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "failed = verify_images(get_image_files(Path('cropped_images')))\n",
    "failed.map(Path.unlink)\n",
    "len(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: No NaN values found in 'target' column (good).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop NaNs and build the initial label dictionary\n",
    "initial_nan_labels_count = df_labels['target'].isna().sum()\n",
    "if initial_nan_labels_count > 0:\n",
    "    print(f\"DEBUG: Found {initial_nan_labels_count} NaN values in 'target' column. Dropping rows with NaN targets.\")\n",
    "    df_labels.dropna(subset=['target'], inplace=True)\n",
    "else:\n",
    "    print(\"DEBUG: No NaN values found in 'target' column (good).\")\n",
    "\n",
    "# Split into train and validation\n",
    "train_df, valid_df = train_test_split(df_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_label_dict = dict(zip(train_df['filename'], train_df['target']))\n",
    "valid_label_dict = dict(zip(valid_df['filename'], valid_df['target']))\n",
    "all_label_dict = {**train_label_dict, **valid_label_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make the datablock. For augmentations, we'll do all except warp (that might make the phyisquese look too different). We can see some of our datablock's examples with show_batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Train labels: 1277 | Valid labels: 320\n",
      "DEBUG: Image path set to: cropped_images\n",
      "DEBUG: Total image files found by get_image_files: 1357\n",
      "DEBUG: Processable image files (with matching labels): 1357\n",
      "DEBUG: Attempting to create DataLoaders...\n",
      "DEBUG: DataLoaders created successfully.\n",
      "DEBUG: Number of training batches: 68\n",
      "DEBUG: Number of validation batches: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"DEBUG: Train labels: {len(train_label_dict)} | Valid labels: {len(valid_label_dict)}\")\n",
    "\n",
    "# Get all image files\n",
    "path = Path('cropped_images')\n",
    "print(f\"DEBUG: Image path set to: {path}\")\n",
    "\n",
    "all_image_files = get_image_files(path)\n",
    "print(f\"DEBUG: Total image files found by get_image_files: {len(all_image_files)}\")\n",
    "\n",
    "# Filter image files to only those with matching labels\n",
    "processable_image_files = [f for f in all_image_files if f.name in all_label_dict]\n",
    "print(f\"DEBUG: Processable image files (with matching labels): {len(processable_image_files)}\")\n",
    "\n",
    "# Safety check\n",
    "if len(processable_image_files) == 0:\n",
    "    print(\"CERROR: No processable image files found (no images match labels or vice-versa).\")\n",
    "    if all_image_files and all_label_dict:\n",
    "        print(f\"  Sample image file: {all_image_files[0].name}\")\n",
    "        print(f\"  Sample label key: {next(iter(all_label_dict.keys()))}\")\n",
    "        if all_image_files[0].name not in all_label_dict and all_image_files[0].name.split('.')[0] in [k.split('.')[0] for k in all_label_dict.keys()]:\n",
    "            print(\" Filename extensions might differ between image files and label keys.\")\n",
    "    raise ValueError(\"Cannot create DataLoaders: No matching image files and labels.\")\n",
    "\n",
    "# Helper function to get label\n",
    "def get_y_func(fn):\n",
    "    key = fn.name\n",
    "    if key not in all_label_dict:\n",
    "        print(f\"DEBUG ERROR: Label not found for: {key} during get_y_func call. This should not happen if pre-filtered.\")\n",
    "        raise ValueError(f\"Label not found for: {key}\")\n",
    "    return all_label_dict[key]\n",
    "\n",
    "# Generate index lists for DataBlock IndexSplitter\n",
    "filename_to_index = {f.name: i for i, f in enumerate(processable_image_files)}\n",
    "valid_idxs = [filename_to_index[fname] for fname in valid_df['filename'] if fname in filename_to_index]\n",
    "splitter = IndexSplitter(valid_idxs)\n",
    "\n",
    "def convert_to_rgb(img):\n",
    "    return img.convert('RGB')\n",
    "\n",
    "# Transformations\n",
    "item_tfms = Resize(244, method=\"pad\")\n",
    "\n",
    "batch_tfms = aug_transforms(\n",
    "    do_flip=True,\n",
    "    max_rotate=2,     \n",
    "    max_zoom=1.05,    \n",
    "    max_lighting=0.1, \n",
    "    max_warp=0.,\n",
    "    p_affine=0.3,     \n",
    "    p_lighting=0.3   \n",
    ")\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, RegressionBlock),\n",
    "    get_items=lambda _: processable_image_files,\n",
    "    splitter=splitter,\n",
    "    get_y=get_y_func,\n",
    "    item_tfms=item_tfms,\n",
    "    batch_tfms=batch_tfms,\n",
    "    n_inp=1\n",
    ")\n",
    "\n",
    "print(\"DEBUG: Attempting to create DataLoaders...\")\n",
    "try:\n",
    "    dls = dblock.dataloaders(path, bs=16)\n",
    "    print(\"DEBUG: DataLoaders created successfully.\")\n",
    "    print(f\"DEBUG: Number of training batches: {len(dls.train)}\")\n",
    "    print(f\"DEBUG: Number of validation batches: {len(dls.valid)}\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Failed to create DataLoaders: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Create model with correct input size\n",
    "model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=1)\n",
    "\n",
    "# Create learner with the pre-configured model\n",
    "learn = Learner(dls, model, metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>42.501873</td>\n",
       "      <td>143.486038</td>\n",
       "      <td>11.978566</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.112394</td>\n",
       "      <td>12.197266</td>\n",
       "      <td>3.492458</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.454126</td>\n",
       "      <td>10.838336</td>\n",
       "      <td>3.292163</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.753793</td>\n",
       "      <td>14.980948</td>\n",
       "      <td>3.870523</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.135998</td>\n",
       "      <td>31.817371</td>\n",
       "      <td>5.640689</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.226499</td>\n",
       "      <td>3602.582764</td>\n",
       "      <td>60.021519</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.059509</td>\n",
       "      <td>18.491421</td>\n",
       "      <td>4.300165</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.461150</td>\n",
       "      <td>11.648339</td>\n",
       "      <td>3.412966</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.213525</td>\n",
       "      <td>13.621491</td>\n",
       "      <td>3.690731</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.447729</td>\n",
       "      <td>8.372024</td>\n",
       "      <td>2.893445</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.955762</td>\n",
       "      <td>18.085747</td>\n",
       "      <td>4.252734</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.376658</td>\n",
       "      <td>8.983667</td>\n",
       "      <td>2.997277</td>\n",
       "      <td>00:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.874877</td>\n",
       "      <td>9.096365</td>\n",
       "      <td>3.016018</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.199475</td>\n",
       "      <td>15.986969</td>\n",
       "      <td>3.998371</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.030691</td>\n",
       "      <td>9.005800</td>\n",
       "      <td>3.000966</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.503890</td>\n",
       "      <td>11.377701</td>\n",
       "      <td>3.373085</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.116205</td>\n",
       "      <td>8.439627</td>\n",
       "      <td>2.905103</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.137115</td>\n",
       "      <td>7.814407</td>\n",
       "      <td>2.795426</td>\n",
       "      <td>00:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.864483</td>\n",
       "      <td>7.628496</td>\n",
       "      <td>2.761973</td>\n",
       "      <td>00:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.721653</td>\n",
       "      <td>7.685026</td>\n",
       "      <td>2.772188</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.621064</td>\n",
       "      <td>7.797533</td>\n",
       "      <td>2.792406</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a working model! For example, it predicts this picture at 13% bodyfat (not so far off in my opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bodyfat prediction: 14.5279\n"
     ]
    }
   ],
   "source": [
    "bf,_,probs = learn.predict(PILImage.create('images/248_image_2.jpg'))\n",
    "print(f\"Bodyfat prediction: {probs[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.3 (v3.13.3:6280bb54784, Apr  8 2025, 10:47:54) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "Name: fastai\n",
      "Version: 2.8.2\n",
      "Summary: fastai simplifies training fast and accurate neural nets using modern best practices\n",
      "Home-page: https://github.com/fastai/fastai\n",
      "Author: Jeremy Howard, Sylvain Gugger, and contributors\n",
      "Author-email: info@fast.ai\n",
      "License: Apache Software License 2.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages\n",
      "Requires: cloudpickle, fastcore, fastdownload, fastprogress, fasttransform, matplotlib, packaging, pandas, pillow, pip, plum-dispatch, pyyaml, requests, scikit-learn, scipy, spacy, torch, torchvision\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "!pip3 show fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7594816,
     "sourceId": 12076934,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
